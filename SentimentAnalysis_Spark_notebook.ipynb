{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up spark, java and hadoop environments\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = 'C:\\spark\\spark-3.2.0-bin-hadoop3.2'\n",
    "os.environ['JAVA_HOME'] = 'C:\\Program Files\\Java\\jdk1.8.0_301'\n",
    "os.environ['HADOOP_HOME'] = 'C:\\hadoop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139932f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e130424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(lines):\n",
    "    #\n",
    "    words = lines.select(explode(split(lines.value, \"t_end\")).alias(\"word\"))\n",
    "    \n",
    "    # NaN values were removed\n",
    "    words = words.na.replace('', None)\n",
    "    words = words.na.drop()\n",
    "    \n",
    "    # below https, single characters, hashtags, RTs are removed from the tweets\n",
    "    words = words.withColumn('word', F.regexp_replace('word', r'http\\S+', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', '@\\w+', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', '#', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', 'RT', ''))\n",
    "    words = words.withColumn('word', F.regexp_replace('word', ':', ''))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dba3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text classification\n",
    "def polarity_detection(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def subjectivity_detection(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "def text_classification(words):\n",
    "    # polarity detection\n",
    "    polarity_detection_udf = udf(polarity_detection, StringType())\n",
    "    words = words.withColumn(\"polarity\", polarity_detection_udf(\"word\"))\n",
    "    # subjectivity detection\n",
    "    subjectivity_detection_udf = udf(subjectivity_detection, StringType())\n",
    "    words = words.withColumn(\"subjectivity\", subjectivity_detection_udf(\"word\"))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534020d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # create Spark session\n",
    "    spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n",
    "    # read the tweet data from socket\n",
    "    lines = spark.readStream.format(\"socket\").option(\"host\", \"127.0.0.1\").option(\"port\", 5555).load()\n",
    "    # Preprocess the data\n",
    "    words = preprocessing(lines)\n",
    "    # text classification to define polarity and subjectivity\n",
    "    words = text_classification(words)\n",
    "    words = words.repartition(1)\n",
    "    query = words.writeStream.queryName(\"all_tweets\")\\\n",
    "        .outputMode(\"append\").format(\"parquet\")\\\n",
    "        .option(\"path\", \"./parc\")\\\n",
    "        .option(\"checkpointLocation\", \"./check\")\\\n",
    "        .trigger(processingTime='60 seconds').start()\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe4313",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyarrow\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ccb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# the parquet files are read into a single dataframe for analysis\n",
    "data_dir = Path('parc')\n",
    "tweets_df = pd.concat(\n",
    "    pd.read_parquet(parquet_file)\n",
    "    for parquet_file in data_dir.glob('*.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3976e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c6d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b490b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Negative, Positive, Neutral and Compound values\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "for index, row in tweets_df['word'].iteritems():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    \n",
    "    if neg > pos:\n",
    "        tweets_df.loc[index, 'sentiment'] = \"negative\"\n",
    "    elif pos > neg:\n",
    "        tweets_df.loc[index, 'sentiment'] = \"positive\"\n",
    "    else:\n",
    "        tweets_df.loc[index, 'sentiment'] = \"neutral\"\n",
    "        tweets_df.loc[index, 'neg'] = neg\n",
    "        tweets_df.loc[index, 'neu'] = neu\n",
    "        tweets_df.loc[index, 'pos'] = pos\n",
    "        tweets_df.loc[index, 'compound'] = comp\n",
    "\n",
    "tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac34013f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6313bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "\n",
    "#Count_values for sentiment\n",
    "count_values_in_column(tweets_df,\"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# create data for Pie Chart\n",
    "pichart = count_values_in_column(tweets_df,\"sentiment\")\n",
    "names= pichart.index\n",
    "size=pichart[\"Percentage\"]\n",
    " \n",
    "# Create a circle for the center of the plot\n",
    "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
    "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
    "p=plt.gcf()\n",
    "p.gca().add_artist(my_circle)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Create Wordcloud\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "\n",
    "def create_wordcloud(text):\n",
    "    #mask = np.array(Image.open(\"cloud.png\"))\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(background_color=\"white\",\n",
    "                   mask = None,\n",
    "                   max_words=3000,\n",
    "                   stopwords=stopwords,\n",
    "                   repeat=True)\n",
    "    wc.generate(str(text))\n",
    "    wc.to_file(\"wc.png\")\n",
    "    \n",
    "    print(\"Word Cloud Saved Successfully\")\n",
    "    \n",
    "    path=\"wc.png\"\n",
    "    display(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25b90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df['subjectivity'] = tweets_df['subjectivity'].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for all tweets\n",
    "create_wordcloud(tweets_df[\"word\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb27da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for positive tweets\n",
    "positive_tweets = tweets_df[tweets_df['sentiment']=='positive']\n",
    "print('Average people subjectivity for positive tweets: ', positive_tweets['subjectivity'].mean())\n",
    "create_wordcloud(positive_tweets[\"word\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892168ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for negative tweets\n",
    "negative_tweets = tweets_df[tweets_df['sentiment']=='negative']\n",
    "print('Average people subjectivity for negative tweets: ', negative_tweets['subjectivity'].mean())\n",
    "create_wordcloud(negative_tweets[\"word\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e438c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating wordcloud for neutral tweets\n",
    "neutral_tweets = tweets_df[tweets_df['sentiment']=='neutral']\n",
    "print('Average people subjectivity: ', neutral_tweets['subjectivity'].mean())\n",
    "create_wordcloud(neutral_tweets[\"word\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214f3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet’s lenght and word count\n",
    "tweets_df['text_len'] = tweets_df['word'].astype(str).apply(len)\n",
    "tweets_df['text_word_count'] = tweets_df['word'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tweets_df.groupby(\"sentiment\").text_len.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0f166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tweets_df.groupby(\"sentiment\").text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import nltk \n",
    "\n",
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0–9]+', '', text)\n",
    "    return text\n",
    "\n",
    "tweets_df['punct'] = tweets_df['word'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "tweets_df['tokenized'] = tweets_df['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "tweets_df['nonstop'] = tweets_df['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#Appliyng Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "tweets_df['stemmed'] = tweets_df['nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#Appliyng Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tweets_df['word'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
    "#print(countVectorizer.get_feature_names())\n",
    "\n",
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aa634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Used Words\n",
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "#n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tweets_df['word'],(2,2),20)\n",
    "n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474fee40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#n3_trigram\n",
    "n3_trigrams = get_top_n_gram(tweets_df['word'],(3,3),20)\n",
    "n3_trigrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
